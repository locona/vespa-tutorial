////
Copyright 2018 Yahoo Japan Corporation.
Licensed under the terms of the MIT license.
See LICENSE in the project root.
////

[[clustering]]
= Vespa とクラスタリング
include::_include.adoc[]

このセクションでは、複数ノードを用いて Vespa をクラスタリングする方法について見ていきます。

[[clustering_setup]]
== クラスタの構築

本チュートリアルでは、実際に3つのノードを用いた Vespa クラスタを構築します。
対応する設定は
`sample-apps/config/cluster`
にあります。

[[clustering_setup_tutorial]]
=== チュートリアルでの構成

構築する Vespa クラスタの構成を図にすると以下のようになります。

image::vespa_tutorial_cluster.jpg[width=900, align="center"]

[NOTE]
====
上図は役割軸でコンポーネントをざっくり書いたもので、ブロックと実際のプロセスが対応しているわけではない点に注意してください。

なお、実際にどのノードで何のプロセスが起動するかは
http://docs.vespa.ai/documentation/reference/files-processes-and-ports.html[Files, processes and ports]
にまとめられています。
====

Vespa クラスタの構築で修正が必要となるのは `hosts.xml` と `services.xml` の2つです。

[[clustering_setup_tutorial_hosts]]
==== hosts.xml

`hosts.xml` の具体的な中身は以下のようになります。

[source, xml]
----
<?xml version="1.0" encoding="utf-8" ?>
<hosts>
  <host name="vespa1">
    <alias>node1</alias>
  </host>

  <host name="vespa2"> <1>
    <alias>node2</alias>
  </host>

  <host name="vespa3"> <2>
    <alias>node3</alias>
  </host>
</hosts>
----

<1> `vespa2` のホストを追加
<2> `vespa3` のホストを追加

<<2_config.adoc#config,Vespa の設定>>
で用いたシングル構成と比べて、指定されているホスト名が増えています。

[[clustering_setup_tutorial_services]]
==== services.xml

`services.xml` の具体的な中身は以下のようになります。

[source, xml]
----
<?xml version="1.0" encoding="utf-8" ?>
<services version="1.0">

  <admin version="2.0">
    <adminserver hostalias="node1"/>
    <configservers>
      <configserver hostalias="node1"/>
    </configservers>
    <logserver hostalias="node1"/>
    <slobroks>
      <slobrok hostalias="node1"/>
    </slobroks>
  </admin>

  <container id="container" version="1.0">
    <component id="jp.co.yahoo.vespa.language.lib.kuromoji.KuromojiLinguistics"
               bundle="kuromoji-linguistics">
      <config name="language.lib.kuromoji.kuromoji">
        <mode>search</mode>
        <ignore_case>true</ignore_case>
      </config>
    </component>
    <document-api/>
    <document-processing/>
    <search/>
    <nodes>
      <node hostalias="node1"/>
      <node hostalias="node2"/>
      <node hostalias="node3"/>
    </nodes>
  </container>

  <content id="book" version="1.0">
    <redundancy>2</redundancy> <1>
    <documents>
      <document type="book" mode="index"/>
      <document-processing cluster="container"/>
    </documents>
    <nodes>
      <node hostalias="node1" distribution-key="0"/>
      <node hostalias="node2" distribution-key="1"/> <2>
      <node hostalias="node3" distribution-key="2"/> <3>
    </nodes>
  </content>

</services>
----

<1> ドキュメントの冗長数を `2` に変更
<2> `vespa2` のホストを追加
<3> `vespa3` のホストを追加

こちらもシングル構成に比べて、`container` と `content` の `nodes` セクションの定義が増えていることがわかります。
今回は3つのノードで同じ設定を利用するため、定義の追加はこれだけで OK です。
また、それに加えて、クラスタ設定ではドキュメントの冗長数を `1` から `2` に増やしています。

[IMPORTANT]
====
<<2_config.adoc#config_file_services_content,content>>
で述べたように、`content` セクションに追加した `node` は全て異なる `distribution-key` を設定する必要があります。
====

[[clustering_setup_deploy]]
=== クラスタ設定の反映

[IMPORTANT]
====
すでにシングル構成での Vespa が起動して、データまで投入されていることを前提としています。
====

シングル構成では、`vespa1` のノードに13件のドキュメントが登録されているという状態でした。
チュートリアルで用意している `utils/vespa_cluster_status` を以下のように実行すると、
`vespa1` (`node=0`) に13件のドキュメントがいることが確認できます。

[source, bash]
----
$ utils/vespa_cluster_status -t storage book
=== status of storage ===

| node | status      | bucket-count | uniq-doc-count | uniq-doc-size |
|------|-------------|--------------|----------------|---------------|
|    0 | up          |           13 |             13 |          9008 |
----

[TIP]
====
`utils/vespa_cluster_status` は
http://docs.vespa.ai/documentation/content/api-state-rest-api.html[State Rest API]
を用いて `content` ノードの状態を確認しています。
====

次に、以下のコマンドでクラスタの設定を Vespa にデプロイします。

[source, bash]
----
$ sudo docker-compose exec vespa1 /bin/bash <1>

[root@vespa1 /]# vespa-deploy prepare /vespa-sample-apps/config/cluster/ <2>
Uploading application '/vespa-sample-apps/config/cluster/' using http://vespa1:19071/application/v2/tenant/default/session?name=cluster
Session 4 for tenant 'default' created.
Preparing session 4 using http://vespa1:19071/application/v2/tenant/default/session/4/prepared
WARNING: Host named 'vespa2' may not receive any config since it does not match its canonical hostname: vespa2.vespatutorial_vespa-nw
WARNING: Host named 'vespa3' may not receive any config since it does not match its canonical hostname: vespa3.vespatutorial_vespa-nw
Session 4 for tenant 'default' prepared.

[root@vespa1 /]# vespa-deploy activate <3>
Activating session 4 using http://vespa1:19071/application/v2/tenant/default/session/4/active
Session 4 for tenant 'default' activated.
Checksum:   c170db03dc38f7f53d9b98aa89d782de
Timestamp:  1519282647370
Generation: 4
----

<1> `vespa1` の Docker コンテナにログイン
<2> `/vespa-sample-apps/config/cluster/` を Vespa にアップロード
<3> 最新の設定を Vespa に反映

[NOTE]
====
チュートリアルでは `hosts.xml` のホスト名が若干雑なので `WARNING` がでていますが、
ノード間で通信はできているため動作上は問題ありません。
====

チュートリアル付属の `utils/vespa_status` を実行すると、
`vespa2` と `vepsa3` も `OK` になっていることがわかります。

[source, bash]
----
$ utils/vespa_status
configuration server ...   [ OK ]
application server (vespa1) ...   [ OK ]
application server (vespa2) ...   [ OK ]
application server (vespa3) ...   [ OK ]
----

また、先程の `utils/vespa_cluster_status` を実行すると、
ドキュメントが3つのノードに分散され、
さらに冗長数が `2` になるようにコピーが行われている (総数が26件になっている) ことが確認できます。

[source, bash]
----
$ utils/vespa_cluster_status -t storage book
=== status of storage ===

| node | status      | bucket-count | uniq-doc-count | uniq-doc-size |
|------|-------------|--------------|----------------|---------------|
|    0 | up          |            7 |              7 |          5079 |
|    1 | up          |           11 |             11 |          7665 |
|    2 | up          |            8 |              8 |          5272 |
----

[NOTE]
====
実際にドキュメントの情報が `State REST API` の結果に反映されるまで少し時間がかかります。
====

実際に検索すると、初めに登録していた13件のドキュメントが取得できることがわかります。

[source, bash]
----
$ curl 'http://localhost:8080/search/?query=sddocname:book'
{"root":{"id":"toplevel","relevance":1.0,"fields":{"totalCount":13}, ...
----

[[clustering_state]]
== クラスタの状態管理

Vespa のクラスタを運用する上で、クラスタ内の各ノードの状態を把握することが重要です。
ここでは、Vespa で提供されているコマンドを用いてクラスタの状態を管理する手順について説明します。

[[clustering_state_howto]]
=== 状態管理の仕組み

Vespa では
http://docs.vespa.ai/documentation/clustercontroller.html[Cluster Controller]
と呼ばれるコンポーネントが各ノードの状態を管理しています。
`Cluster Controller` は `services.xml` の
<<2_config.adoc#config_file_services_admin,admin>> セクションの `cluster-controllers` で定義していたサーバのことで、
Vespa クラスタ全体の状態管理を担当しています。

[TIP]
====
実際にはチュートリアルの設定では `cluster-controllers` を明示的に定義していません。
`cluster-controllers` の定義が無い場合は `configservers` のノードが `Cluster Controller` を兼務します。
====

http://docs.vespa.ai/documentation/img/design/clustercontroller-overview.png[公式ドキュメントの図]
のように、`Cluster Controller` の動作の流れは以下の通りです。

. `slobrok` (Service Location Broker) からノードのリストを取得
. 各ノードに対して現在の状態を問い合わせ (`u/d/m/r` は後述のノード状態に対応)
. 得られた情報から最終的なクラスタの状態を更新して全体に通知

チュートリアルの例では `Cluster Controller` は一つのみ指定していますが、
複数指定して冗長構成を取ることも可能です。

[TIP]
====
http://docs.vespa.ai/documentation/slobrok.html[slobrok]
は各サービスがどこのホストのどのポートで動作しているかを管理するコンポーネントです。

http://docs.vespa.ai/documentation/reference/vespa-cmdline-tools.html#vespa-model-inspect[vespa-model-inspect]
コマンドを用いると `slobrok` に問い合わせることができ、
以下のように指定したサービスのホストと対応するポート番号を取得できます。

[source, bash]
----
[root@vespa1 /]# vespa-model-inspect service container
container @ vespa1 :
container/container.0
    tcp/vespa1:8080 (STATE EXTERNAL QUERY HTTP)
    tcp/vespa1:19100 (EXTERNAL HTTP)
    tcp/vespa1:19101 (MESSAGING RPC)
    tcp/vespa1:19102 (ADMIN RPC)
container @ vespa2 :
container/container.1
    tcp/vespa2:8080 (STATE EXTERNAL QUERY HTTP)
    tcp/vespa2:19100 (EXTERNAL HTTP)
    tcp/vespa2:19101 (MESSAGING RPC)
    tcp/vespa2:19102 (ADMIN RPC)
container @ vespa3 :
container/container.2
    tcp/vespa3:8080 (STATE EXTERNAL QUERY HTTP)
    tcp/vespa3:19100 (EXTERNAL HTTP)
    tcp/vespa3:19101 (MESSAGING RPC)
    tcp/vespa3:19102 (ADMIN RPC)
----
====

[IMPORTANT]
====
`Cluster Controller` を冗長構成にした場合、
内部では `Cluster Controller` のどれか一つがマスターとなって状態管理を行います。

マスターの選別は `Cluster Controller` 全体での投票により行われ、
過半数以上の票を集めた `Cluster Controller` がマスターとなります。
このため、例えば `N` プロセスまでのダウンを許容するようにシステムを構築する場合、
 過半数を担保するため `Cluster Controller` の総数は `2 * N + 1` プロセスとする必要があるので注意してください。
====

[[clustering_state_node]]
=== ノードの状態

Vespa クラスタのノードは
http://docs.vespa.ai/documentation/content/admin-states.html[Cluster and node states]
にあるように6つの状態を取りますが、運用上で使うのは起動処理中と停止処理中を除いた以下の4つです。

[options="header", cols="2,2,1,1"]
|====
^| 状態 ^| 意味 ^| 分散検索の対象? ^| 分散配置の対象?
| up | サービスが提供可能。 ^| o ^| o
| down | サービスが提供不可能。 ^| x ^| x
| maintenance | メンテナンス中。 ^| x ^| o
| retired | 退役済み。 ^| o ^| x
|====

[NOTE]
====
4つと言いましたが、実運用では `up` と `down` の2つだけあればだいたいなんとかなります。
====

表のように、4つはドキュメントの分散検索および分散配置の対象となるかどうか、という観点で動作が異なります。

`up` と `down` は非常にシンプルで、それぞれ全ての対象となるか完全に除外されるかに対応します。

`maintenance` は分散検索の対象からは外れますが、分散配置の対象にはなる、という動作をします。
`maintenance` はソフトウェアの更新作業など、比較的短いメンテナンス作業を行うときの指定が想定された状態で、
少しの間だけなので状態更新に伴うドキュメント再分散の負荷を避けたい、というときに利用します。

`retired` は分散検索の対象は維持しますが、分散配置の対象から外れる、という動作をします。
`retired` では分散配置の対象から外れるため、
状態更新後にノード上にあるドキュメントは0件になるまで (低優先度で) クラスタ内の他のノードへの再分配されていきます。
一方で、`retired` では分散検索の対象にはなるため、
サービスは提供しながら徐々に担当ドキュメントを別ノードへ逃がしていき、
最終的に0件となったところで役割を終えて引退させる、というような使い方をします。

各ノードの状態は `vespa-get-node-state` というコマンドを用いて取得できます。

[source, bash]
----
[root@vespa1 /]# vespa-get-node-state -i 0 <1>
Shows the various states of one or more nodes in a Vespa Storage cluster. There
exist three different type of node states. They are:

  Unit state      - The state of the node seen from the cluster controller.
  User state      - The state we want the node to be in. By default up. Can be
                    set by administrators or by cluster controller when it
                    detects nodes that are behaving badly.
  Generated state - The state of a given node in the current cluster state.
                    This is the state all the other nodes know about. This
                    state is a product of the other two states and cluster
                    controller logic to keep the cluster stable.

book/distributor.0: <2>
Unit: up:
Generated: up:
User: up:

book/storage.0: <3>
Unit: up:
Generated: up:
User: up:
----

<1> `distribution-key=0` のノード (`-i 0`) の状態を取得
<2> `distributor` (ドキュメント分散を行うコンポーネント) の状態
<3> `storage` (ドキュメント保持を行うコンポーネント) の状態

また、クラスタ全体の状態は `vepsa-get-cluster-state` というコマンドで確認できます。

[source, bash]
----
[root@vespa1 /]# vespa-get-cluster-state

Cluster book:
book/distributor/0: up
book/distributor/1: up
book/distributor/2: up
book/storage/0: up
book/storage/1: up
book/storage/2: up
----

[[clustering_state_check]]
=== ノード状態の判定

Vespa では、ノードの状態は

* システム的に判断した状態 (`Unit state`)
* ユーザが明示的に指定した状態 (`User state`)

の2つの情報から最終状態 (`Generated state`) が判断されます。

[[clustering_state_check_unit]]
==== Unit state

`Unit state` はプロセスの状態から機械的に判断される状態のことです。

例えば以下のように `vespa2` の Docker コンテナを停止されると、
`vespa2` に対応する `distribution-key=1` のノードの `Unit` が `down` に更新されます。

[source, bash]
----
$ sudo docker-compose stop vespa2 <1>
Stopping vespa2 ... done

$ sudo docker-compose exec vespa1 /bin/bash <2>

[root@vespa1 /]# vespa-get-node-state -i 1 <3>
...
book/distributor.1:
Unit: down: Connection error: Closed at other end. (Node or switch likely shutdown) <4>
Generated: down: Connection error: Closed at other end. (Node or switch likely
shut down)
User: up:

book/storage.1:
Unit: down: Connection error: Closed at other end. (Node or switch likely shutdown) <5>
Generated: down: Connection error: Closed at other end. (Node or switch likely
shut down)
User: up:
----

<1> `vespa2` の Docker コンテナを停止
<2> `vespa1` の Docker コンテナにログイン
<3> `vespa2` (`distribution-key=1`) の状態を確認
<4> `distributor` の `Unit state` が `down` に変化
<5> `storage` の `Unit state` が `down` に変化

[[clustering_state_check_user]]
==== User state

`User state` はユーザが明示的に指定したノードの状態のことで、
`vespa-set-node-state` コマンドを利用して指定します。

例えば、`vespa3` ノードのハードウェアが不調で、Vespa クラスタから明示的に外す (`down` にする) 場合は、
以下のようなコマンドとなります。

[source]
----
[root@vespa1 /]# vespa-set-node-state -i 2 down "hardware trouble" <1>
{"wasModified":true,"reason":"ok"}OK
{"wasModified":true,"reason":"ok"}OK

[root@vespa1 /]# vespa-get-node-state -i 2 <2>
...
book/distributor.2:
Unit: up:
Generated: down: hardware trouble
User: down: hardware trouble <3>

book/storage.2:
Unit: up:
Generated: down: hardware trouble
User: down: hardware trouble <4>
----

<1> `vespa3` (`distribution-key=2`) の状態を `hardware trouble` という理由で `down` に変更
<2> `vespa3` (`distribution-key=2`) の状態を確認
<3> `distributor` の `User state` が `down` に変化
<4> `storage` の `User state` が `down` に変化

なお、`vespa-set-node-state` では上記例のように、第2引数でメモを付けることができます (省略も可)。

[TIP]
====
ノードの状態を明示的に変更することは、
例えば不調なノードが意図せず起動してしまったときに Vespa クラスタに参加させないようにする、
といった意図しない状態更新を抑止する効果があります。
====

[NOTE]
====
`maintenance` の状態は `storage` にしか指定できません。
これは、`distributor` を `maintenance` にすると、ドキュメント分散自体が止まる可能性があるためです。

`vespa-set-node-state` で特定のコンポーネントを指定する場合は、
以下のように `-t` オプションで指定します。

[source]
----
[root@vespa1 /]# vespa-get-node-state -i 2 -t storage maintenance "update software"
----
====

[[clustering_distrib]]
== ドキュメントの分散

Vespa ではドキュメントを
http://docs.vespa.ai/documentation/content/buckets.html[bucket]
と呼ばれる単位に分割されて管理されます。
`bucket` という細かい粒度でドキュメントの冗長性を管理することで、
Vespa ではノード増減に対する高い柔軟性を担保しています。

[TIP]
====
公式ドキュメントでは
http://docs.vespa.ai/documentation/elastic-vespa.html[Vespa elasticity]
にVespaの柔軟性に関する情報がまとめられています。
====

[[clustering_distrib_process]]
=== Distributor と SearchNode

ドキュメントの分散は、ノード状態の話で名前のでてきた `Distributor` と `Storage` という2つのコンポーネントが関わってきます。
なお、
http://docs.vespa.ai/documentation/elastic-vespa.html[公式ドキュメント]
では `Storage` は `SearchNode` と呼ばれているため、
ここでは `SearchNode` と呼んでいきます。

[TIP]
====
より具体的なプロセス名だと、
`Distributor` は `vespa-distributord-bin` に、
`SearchNode` は `vespa-proton-bin` に対応します。
====

`Distributor` はドキュメントの分散を担当するコンポーネントで、
各 `bucket` のチェックサムや割り振り先などのクラスタ全体での `bucket` の一貫性の担保に関わる情報を管理しています。
`Distributor` は `services.xml` で指定された冗長数と `Cluster Controller` から得られるクラスタの状態を元に、
各 `bucket` がどこに配置されるべきか、再分配が必要なのか、といったことを判断します。

`SearchNode` は実際にインデックスへのアクセスを担当するコンポーネントで、
`bucket` の本体を保持しています。
`SearchNode` では Vespa の永続化や検索といった検索エンジンのコア実装が含まれています。

[TIP]
====
`SearchNode` のより細かい話は
http://docs.vespa.ai/documentation/proton.html[Proton]
を参照してください。
====

[[clustering_distrib_bucket]]
=== bucket の配置と再分配

`bucket` の配置は、以下のように各ノードに対して乱数を生成し、
それをソートした順番をもとに優先順位を決めることで行われます。

image::vespa_bucket_distrib.jpg[width=700, align="center"]

[TIP]
====
分散アルゴリズムのより詳しい話は
http://docs.vespa.ai/documentation/content/idealstate.html[Distribution algorithm]
を参照してください。
====

得られたノード番号 (`distribution-key`) 列のうち、一番先頭のノードに `bucket` のプライマリコピーが、
冗長数に応じてそれ以降のノードに `bucket` のセカンダリコピーが配置されます。
Vespa ではこのルールに従い、ノードが増減したときの `bucket` の再分配が行われます。
例えば、冗長数が `2` の状態でノードが増減したときに `bucket` の動きは以下のようになります。

image::vespa_bucket_redistrib.jpg[width=800, align="center"]

[IMPORTANT]
====
各 `bucket` に割り当てられるシーケンスは常に同じ結果になります。
そのため、
<<2_config.adoc#config_file_services_content,content>>
にある「各ノードに割り振られた `distribution-key` が変わらないようにすること」が重要となるわけです。
====

[[clustering_distrib_example]]
=== チュートリアル環境での例

チュートリアルの Vespa クラスタを用いて、
実際にノードを増減させた場合にどのような動作をするか見ていきます。
クラスタにドキュメントを全件追加したときの状態は以下のようになっていました。

[source, bash]
----
$ utils/vespa_cluster_status -t storage book
=== status of storage ===

| node | status      | bucket-count | uniq-doc-count | uniq-doc-size |
|------|-------------|--------------|----------------|---------------|
|    0 | up          |            7 |              7 |          5079 |
|    1 | up          |           11 |             11 |          7665 |
|    2 | up          |            8 |              8 |          5272 |
----

ここで、試しに `vespa2` (上図の `node=1`) の Docker コンテナを停止させると、
`vespa2` のドキュメントが他の2つのノードに分配されることが確認できます。

[source, bash]
----
$ sudo docker-compose stop vespa2 <1>
Stopping vespa2 ... done

$ utils/vespa_cluster_status -t storage book <2>
=== status of storage ===

| node | status      | bucket-count | uniq-doc-count | uniq-doc-size |
|------|-------------|--------------|----------------|---------------|
|    0 | up          |           13 |             13 |          9008 |
|    1 | down        |            0 |              0 |             0 |
|    2 | up          |           13 |             13 |          9008 |
----

<1> `vespa2` の Docker コンテナを停止
<2> `vespa2` (`node=1`) が担当していた11件が他の2ノードに再分配

[NOTE]
====
停止させた直後は `node=1` の状態が `maintenance` に代わり、
しばらくして `down` になります。
====

次に、停止させた `vepsa2` を再び起動させると、
3つのノードに再分配されて元の状態に戻ることが確認できます。

[source, bash]
----
$ sudo docker-compose start vespa2 <1>
Starting vespa2 ... done

$ utils/vespa_cluster_status -t storage book <2>
=== status of storage ===

| node | status      | bucket-count | uniq-doc-count | uniq-doc-size |
|------|-------------|--------------|----------------|---------------|
|    0 | up          |            7 |              7 |          5079 |
|    1 | up          |           11 |             11 |          7665 |
|    2 | up          |            8 |              8 |          5272 |
----

<1> `vespa2` の Docker コンテナを起動
<2> 3ノードに再分配されて初めの状態に戻る

[[clustering_other]]
== その他のトピック

[[clustering_other_logging]]
=== ログの確認

<<1_setup.adoc#setup_boot,Vespa の起動>>
で紹介した構成図のように、
Vespa では各サービスのログは `log server` に集約されます。
そのため、`log server` のログを参照することで、クラスタ全体のログを確認できます。

[TIP]
====
`log_server` は
<<2_config.adoc#config_file_services_admin,admin>> セクションの `logserver` で定義していたサーバのことです。
====

`log server` では、以下のように `/opt/vespa/logs/vespa/logarchive/` の下にログが集約されています。

[source, bash]
----
[root@vespa1 /]# tree /opt/vespa/logs/vespa/logarchive/
/opt/vespa/logs/vespa/logarchive/
└── 2018
    └── 02
        └── 23
            └── 04-0
----

Vespa のログを見るときは
http://docs.vespa.ai/documentation/reference/logfmt.html[vespa-logfmt]
を使うと便利です。
`vespa-logfmt` は、以下のようにオプションとして多数のログのフィルタが提供されています。

[source, bash]
----
[root@vespa1 /]# vespa-logfmt -h
Usage: /opt/vespa/bin/vespa-logfmt [options] [inputfile ...]
Options:
  -l LEVELLIST  --level=LEVELLIST   select levels to include
  -s FIELDLIST  --show=FIELDLIST    select fields to print
  -p PID        --pid=PID           select messages from given PID
  -S SERVICE    --service=SERVICE   select messages from given SERVICE
  -H HOST       --host=HOST	        select messages from given HOST
  -c REGEX      --component=REGEX   select components matching REGEX
  -m REGEX      --message=REGEX     select message text matching REGEX
  -f            --follow            invoke tail -F to follow input file
  -L            --livestream        follow log stream from logserver
  -N            --nldequote         dequote newlines in message text field
  -t    --tc    --truncatecomponent chop component to 15 chars
  --ts          --truncateservice   chop service to 9 chars

FIELDLIST is comma separated, available fields:
     time fmttime msecs usecs host level pid service component message
Available levels for LEVELLIST:
     fatal error warning info event debug spam
for both lists, use 'all' for all possible values, and -xxx to disable xxx.
----

`vespa-logfmt` を引数なしで実行すると、
デフォルトではノードで起動しているサービスのアプリケーションログに対応する
`/opt/vespa/logs/vespa/vespa.log` が参照されます。

[source, bash]
----
[root@vespa1 /]# vespa-logfmt | tail -3
[2018-02-23 04:14:48.700] INFO    : container-clustercontroller Container.com.yahoo.vespa.clustercontroller.core.database.ZooKeeperDatabase	Fleetcontroller 0: Storing new cluster state version in ZooKeeper: 10
[2018-02-23 04:14:58.612] INFO    : container-clustercontroller Container.com.yahoo.vespa.clustercontroller.core.SystemStateBroadcaster	Publishing cluster state version 10
[2018-02-23 04:46:40.991] INFO    : container-clustercontroller stdout	[GC (Allocation Failure)  232656K->16316K(498112K), 0.0054783 secs]
----

Vespa クラスタ全体のログを見る場合は、引数の `inputfile` として先程のアーカイブログを指定する必要があります。
例えば、`vespa2` ノードの `warning` レベルのログが見たい場合は以下のようなコマンドとなります。

[source, bash]
----
[root@vespa1 /]# vespa-logfmt -l warning -H vespa2 /opt/vespa/logs/vespa/logarchive/2018/02/23/04-0 | tail -3
[2018-02-23 04:14:35.944] WARNING : searchnode       proton.searchlib.docstore.logdatastore	We detected an empty idx file for part '/opt/vespa/var/db/vespa/search/cluster.book/n1/documents/book/2.notready/summary/1519358811014181000'. Erasing it.
[2018-02-23 04:14:35.944] WARNING : searchnode       proton.searchlib.docstore.logdatastore	Removing dangling file '/opt/vespa/var/db/vespa/search/cluster.book/n1/documents/book/1.removed/summary/1519358811013672000.dat'
[2018-02-23 04:14:35.944] WARNING : searchnode       proton.searchlib.docstore.logdatastore	Removing dangling file '/opt/vespa/var/db/vespa/search/cluster.book/n1/documents/book/2.notready/summary/1519358811014181000.dat'
----

[NOTE]
====
チュートリアルの例では時間が標準時間となっていますが、
これは起動している環境のタイムゾーンが `UTC` のためです。

[source, bash]
----
[root@vespa1 /]# date
Fri Feb 23 04:56:06 UTC 2018
----

タイムゾーンを `JST` で実行すれば、ログのタイムスタンプも日本時間になります (なるはず)。
====

[[clustering_other_metrics]]
=== メトリクスの取得

Vespa では、検索レイテンシなどのメトリクスを取得するための
http://docs.vespa.ai/documentation/reference/metrics-health-format.html[Metrics API]
が提供されています。

`Metrics API` が提供されているポートはサービスによって異なります。
各サービスで提供されているポートは
http://docs.vespa.ai/documentation/reference/vespa-cmdline-tools.html#vespa-model-inspect[vespa-model-inspect]
コマンドを使うことで確認できます。

[source, bash]
----
[root@vespa1 /]# vespa-model-inspect service container <1>
container @ vespa1 :
container/container.0
    tcp/vespa1:8080 (STATE EXTERNAL QUERY HTTP) <2>
    tcp/vespa1:19100 (EXTERNAL HTTP)
    tcp/vespa1:19101 (MESSAGING RPC)
    tcp/vespa1:19102 (ADMIN RPC)
container @ vespa2 :
container/container.1
    tcp/vespa2:8080 (STATE EXTERNAL QUERY HTTP)
    tcp/vespa2:19100 (EXTERNAL HTTP)
    tcp/vespa2:19101 (MESSAGING RPC)
    tcp/vespa2:19102 (ADMIN RPC)
container @ vespa3 :
container/container.2
    tcp/vespa3:8080 (STATE EXTERNAL QUERY HTTP)
    tcp/vespa3:19100 (EXTERNAL HTTP)
    tcp/vespa3:19101 (MESSAGING RPC)
    tcp/vespa3:19102 (ADMIN RPC)

[root@vespa1 /]# vespa-model-inspect service searchnode <3>
searchnode @ vespa1 : search
book/search/cluster.book/0
    tcp/vespa1:19109 (STATUS ADMIN RTC RPC)
    tcp/vespa1:19110 (FS4)
    tcp/vespa1:19111 (UNUSED)
    tcp/vespa1:19112 (UNUSED)
    tcp/vespa1:19113 (STATE HEALTH JSON HTTP) <4>
searchnode @ vespa2 : search
book/search/cluster.book/1
    tcp/vespa2:19108 (STATUS ADMIN RTC RPC)
    tcp/vespa2:19109 (FS4)
    tcp/vespa2:19110 (UNUSED)
    tcp/vespa2:19111 (UNUSED)
    tcp/vespa2:19112 (STATE HEALTH JSON HTTP)
searchnode @ vespa3 : search
book/search/cluster.book/2
    tcp/vespa3:19108 (STATUS ADMIN RTC RPC)
    tcp/vespa3:19109 (FS4)
    tcp/vespa3:19110 (UNUSED)
    tcp/vespa3:19111 (UNUSED)
    tcp/vespa3:19112 (STATE HEALTH JSON HTTP)
----

<1> `container` サービス (検索リクエストを受けるところ) のポートを確認
<2> `vespa1` ノードでは `8080` ポート
<3> `searchnode` サービス (インデックスを管理してるところ) のポートを確認
<4> `vespa1` ノードでは `19113` ポート

実際に `Metrics API` を用いると、以下のように `json` 形式でメトリクスが取得できます。

[source, json]
----
[root@vespa1 /]# curl 'http://localhost:8080/state/v1/metrics'
{
    "metrics": {
        "snapshot": {
            "from": 1.519363748005E9,
            "to": 1.519363808005E9
        },
        "values": [
            {
                "name": "search_connections",
                "values": {
                    "average": 0,
                    "count": 1,
                    "last": 0,
                    "max": 0,
                    "min": 0,
                    "rate": 0.016666666666666666
                }
            },
            ...
----

返却されるメトリクスは `snapshot` に書かれた期間でのスナップショットに対応しており、
デフォルトのインターバルは `container` ノードが `300秒` が、
`searchnode` ノードが `60秒` (たぶん) となっています。
各メトリクスは `values` として複数の値を返しますが、共通の項目は以下のように定義されます。

[options="header", cols="1,4"]
|====
^| 項目 ^| 内容
| count | 期間中にメトリクスが計測された回数。
| average | 期間中のメトリクスの平均値 (`sum/count`)。
| rate | 1秒あたりの計測回数 (`count/s`)。
| min | 期間中の最小値。
| max | 期間中の最大値。
| sum | 期間中のメトリクスの総和。
|====

各サービスで色々なメトリクスが出力されますが、例えば以下のようなメトリクスがとれます。

[options="header", cols="1,1,3"]
|====
^| 項目 ^| サービス ^| 内容
| queries | container | クエリの処理数、`rate` がいわゆるQPS。
| query_latency | container | 検索レイテンシの統計値。
| proton.numdocs | searchnode | インデックスされているドキュメント数。
|====

[NOTE]
====
具体的なメトリクス周り公式ドキュメントが
http://docs.vespa.ai/documentation/jdisc/metrics.html#metrics-from-custom-components[このあたり]
しかなく、正直どれがどれに対応しているかはコードを確認する必要があるのが現状です。。。

実際のサービスだとこれに加えて更新リクエストの方もチェックしていましたが、
それは投げる側でモニタリングしていたため、Vespa の `Metrics API` は経由していませんでした。
====

[CAUTION]
====
`container` から返されるメトリクスで
`query_latency`、`mean_query_latency`、`max_query_latency`
の3つは、Vespa側の実装のバグっぽくて全て同じ値になります (`query_latency` と同じ挙動)。

ただ、実際のところ `query_latency` の統計値で3つともわかるため、
残りの2つは冗長な出力に見えます。
====